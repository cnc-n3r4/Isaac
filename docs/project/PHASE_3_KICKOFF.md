# PHASE 3: OPTIMIZATION - IMPLEMENTATION KICKOFF

**Agent Role:** Phase 3 Performance Engineer
**Prerequisites:** Phase 2 COMPLETE (tested & maintainable codebase)
**Timeline:** Week 5-6 (1 week / 80 hours)
**Goal:** Achieve high performance - 5-10x faster overall
**Branch:** `claude/phase-3-optimization-[your-session-id]`

---

## ðŸŽ¯ YOUR MISSION

Transform ISAAC from **tested & maintainable** (8.0/10) to **high performance** (8.5/10) by:
- Implementing async AI operations (10-20x faster batch)
- Parallel plugin loading (60-80% faster startup)
- Advanced caching strategies (50-70% cost reduction)
- Optimizing hot paths (5-10x specific operations)

---

## âœ… PREREQUISITES CHECK

Before starting Phase 3, verify Phase 2 completion:

```bash
# 1. Test coverage â‰¥70%
pytest tests/ --cov=isaac --cov-report=term | grep "TOTAL"
# Should show 70%+

# 2. Command router complexity <10
radon cc isaac/core/command_router.py -nb
# Should show CC <10

# 3. Type hints present
mypy isaac/core/ --ignore-missing-imports
# Should pass with minimal errors

# 4. CI/CD running
# Check .github/workflows/ exists

# 5. Documentation organized
ls docs/
# Should show architecture/, guides/, reference/, project/
```

**If any checks fail, complete Phase 2 first!**

---

## ðŸ“‹ TASK LIST (Execute in Order)

### Task 3.1: Implement Async AI Calls (12 hours)

**Goal:** 10-20x faster for batch AI operations

**3.1.1: Install Async Dependencies (30 min)**

```bash
pip install aiohttp aioboto3 asyncio
pip install pytest-asyncio  # For testing
```

**3.1.2: Convert AI Router to Async (6 hours)**

**Update:** `isaac/ai/router.py`

```python
import asyncio
import aiohttp
from typing import List, Optional

class AIRouter:
    def __init__(self):
        self.cache = QueryCache()
        self.providers = {
            'openai': OpenAIProvider(),
            'claude': ClaudeProvider(),
            'grok': GrokProvider()
        }

    # Keep sync version for backward compatibility
    def query(self, prompt: str, model: str = 'gpt-4') -> str:
        """Synchronous query - runs async in event loop"""
        return asyncio.run(self.query_async(prompt, model))

    # New async version
    async def query_async(self, prompt: str, model: str = 'gpt-4') -> str:
        """Asynchronous query for better performance"""
        # Check cache first
        cached = self.cache.get(prompt, model)
        if cached:
            return cached

        # Query provider asynchronously
        provider = self._select_provider(model)
        response = await provider.query_async(prompt, model)

        # Cache response
        self.cache.set(prompt, model, response)

        return response

    async def batch_query(self, prompts: List[str], model: str = 'gpt-4') -> List[str]:
        """Query multiple prompts concurrently"""
        tasks = [self.query_async(prompt, model) for prompt in prompts]
        return await asyncio.gather(*tasks)

    async def query_with_fallback(self, prompt: str, models: List[str]) -> str:
        """Try multiple models concurrently, return first success"""
        tasks = [self.query_async(prompt, model) for model in models]

        for coro in asyncio.as_completed(tasks):
            try:
                return await coro
            except Exception as e:
                continue  # Try next

        raise Exception("All providers failed")
```

**Update providers:**

**Example:** `isaac/ai/providers/openai_provider.py`
```python
import aiohttp

class OpenAIProvider:
    async def query_async(self, prompt: str, model: str) -> str:
        """Async query to OpenAI API"""
        async with aiohttp.ClientSession() as session:
            async with session.post(
                'https://api.openai.com/v1/chat/completions',
                headers={'Authorization': f'Bearer {self.api_key}'},
                json={
                    'model': model,
                    'messages': [{'role': 'user', 'content': prompt}]
                }
            ) as response:
                data = await response.json()
                return data['choices'][0]['message']['content']
```

**3.1.3: Update Command Router for Async (3 hours)**

**Update:** `isaac/core/command_router.py`

```python
import asyncio
from typing import Optional

class CommandRouter:
    def __init__(self):
        self.ai_router = AIRouter()
        # ... rest of init

    def route_command(self, cmd_str: str, context: Dict) -> CommandResult:
        """Sync wrapper - maintains backward compatibility"""
        # Check if command needs AI
        if self._requires_ai_validation(cmd_str):
            # Run async in event loop
            return asyncio.run(self._route_with_ai_async(cmd_str, context))
        else:
            return self._route_direct(cmd_str, context)

    async def _route_with_ai_async(self, cmd_str: str, context: Dict) -> CommandResult:
        """Async routing with AI validation"""
        # AI validation
        is_safe = await self.ai_router.query_async(
            f"Is this command safe to execute: {cmd_str}",
            model='gpt-3.5-turbo'  # Fast model for validation
        )

        if is_safe:
            return self._route_direct(cmd_str, context)
        else:
            return CommandResult(success=False, error="Command blocked by AI")
```

**3.1.4: Test Async Implementation (2 hours)**

**Create:** `tests/test_async_ai.py`

```python
import pytest
import asyncio
from isaac.ai.router import AIRouter

@pytest.mark.asyncio
async def test_query_async():
    router = AIRouter()
    response = await router.query_async("test prompt")
    assert response is not None

@pytest.mark.asyncio
async def test_batch_query():
    router = AIRouter()
    prompts = ["prompt 1", "prompt 2", "prompt 3", "prompt 4", "prompt 5"]

    import time
    start = time.time()
    responses = await router.batch_query(prompts)
    elapsed = time.time() - start

    assert len(responses) == 5
    # Should be much faster than sequential (5x2s = 10s)
    # Concurrent should be ~2-3s
    assert elapsed < 5, f"Batch query too slow: {elapsed}s"

@pytest.mark.asyncio
async def test_fallback_providers():
    router = AIRouter()
    response = await router.query_with_fallback(
        "test",
        models=['gpt-4', 'claude-3', 'grok-1']
    )
    assert response is not None
```

**Run tests:**
```bash
pytest tests/test_async_ai.py -v
```

**Benchmark:**
```bash
# Create benchmark script
cat > benchmark_async.py << 'EOF'
import asyncio
import time
from isaac.ai.router import AIRouter

async def benchmark():
    router = AIRouter()
    prompts = ["prompt " + str(i) for i in range(10)]

    # Sequential
    start = time.time()
    for prompt in prompts:
        await router.query_async(prompt)
    sequential_time = time.time() - start

    # Concurrent
    start = time.time()
    await router.batch_query(prompts)
    concurrent_time = time.time() - start

    print(f"Sequential: {sequential_time:.2f}s")
    print(f"Concurrent: {concurrent_time:.2f}s")
    print(f"Speedup: {sequential_time/concurrent_time:.1f}x")

asyncio.run(benchmark())
EOF

python benchmark_async.py
```

**Commit:**
```bash
git add isaac/ai/ tests/test_async_ai.py
git commit -m "feat: Implement async AI operations (10-20x faster for batch)"
```

**Success:** Batch operations 10-20x faster

---

### Task 3.2: Parallel Plugin Loading (8 hours)

**Goal:** 60-80% faster startup

**3.2.1: Implement Parallel Loader (4 hours)**

**Update:** `isaac/core/boot_loader.py`

```python
import concurrent.futures
from typing import List
import time

class BootLoader:
    def __init__(self):
        self.plugins = []
        self.core_plugins = []  # Must load first
        self.optional_plugins = []  # Can lazy load

    def load_all_plugins(self):
        """Load plugins in parallel"""
        start = time.time()

        # Load core plugins sequentially (fast, dependencies matter)
        for plugin in self.core_plugins:
            self._load_plugin(plugin)

        # Load optional plugins in parallel (slower, no dependencies)
        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            futures = [
                executor.submit(self._load_plugin, plugin)
                for plugin in self.optional_plugins
            ]
            concurrent.futures.wait(futures)

        elapsed = time.time() - start
        print(f"Loaded {len(self.plugins)} plugins in {elapsed:.2f}s")

    def _load_plugin(self, plugin_name: str):
        """Load a single plugin"""
        try:
            module = __import__(f'isaac.commands.{plugin_name}')
            self.plugins.append(module)
        except Exception as e:
            print(f"Failed to load {plugin_name}: {e}")
```

**3.2.2: Implement Lazy Loading (2 hours)**

**Strategy:** Load plugin on first use, not at startup

**Update:** `isaac/core/command_router.py`

```python
class CommandRouter:
    def __init__(self):
        self.loaded_plugins = {}  # Cache loaded plugins
        self.plugin_registry = self._discover_plugins()  # Just scan, don't load

    def _discover_plugins(self) -> Dict[str, str]:
        """Discover available plugins without loading"""
        plugin_dir = 'isaac/commands'
        plugins = {}
        for item in os.listdir(plugin_dir):
            if os.path.isdir(os.path.join(plugin_dir, item)):
                plugins[item] = f'isaac.commands.{item}'
        return plugins

    def _load_plugin_on_demand(self, plugin_name: str):
        """Load plugin only when needed"""
        if plugin_name in self.loaded_plugins:
            return self.loaded_plugins[plugin_name]

        if plugin_name in self.plugin_registry:
            module_path = self.plugin_registry[plugin_name]
            module = __import__(module_path)
            self.loaded_plugins[plugin_name] = module
            return module

        raise PluginNotFoundError(f"Plugin {plugin_name} not found")
```

**3.2.3: Benchmark Startup Performance (2 hours)**

**Create:** `tests/benchmark_startup.py`

```python
import time
from isaac.core.boot_loader import BootLoader

def benchmark_startup():
    # Sequential loading
    loader = BootLoader()
    loader.parallel_loading = False

    start = time.time()
    loader.load_all_plugins()
    sequential_time = time.time() - start

    # Parallel loading
    loader = BootLoader()
    loader.parallel_loading = True

    start = time.time()
    loader.load_all_plugins()
    parallel_time = time.time() - start

    print(f"Sequential: {sequential_time:.2f}s")
    print(f"Parallel: {parallel_time:.2f}s")
    print(f"Improvement: {(1 - parallel_time/sequential_time)*100:.0f}%")

if __name__ == '__main__':
    benchmark_startup()
```

**Run:**
```bash
python tests/benchmark_startup.py
```

**Commit:**
```bash
git add isaac/core/boot_loader.py isaac/core/command_router.py tests/benchmark_startup.py
git commit -m "perf: Implement parallel plugin loading (60-80% faster startup)"
```

**Success:** Startup time reduced from 2-5s to <1s

---

### Task 3.3: Advanced Caching Strategies (12 hours)

**Goal:** Multi-level caching for maximum performance

**3.3.1: Implement Multi-level Cache (4 hours)**

**Create:** `isaac/cache/multilevel_cache.py`

```python
from typing import Any, Optional
import pickle
import os

class LRUCache:
    """Simple LRU cache implementation"""
    def __init__(self, max_size: int):
        self.max_size = max_size
        self.cache = {}
        self.access_order = []

    def get(self, key: str) -> Optional[Any]:
        if key in self.cache:
            # Move to end (most recently used)
            self.access_order.remove(key)
            self.access_order.append(key)
            return self.cache[key]
        return None

    def set(self, key: str, value: Any):
        if key in self.cache:
            self.access_order.remove(key)
        elif len(self.cache) >= self.max_size:
            # Evict least recently used
            lru_key = self.access_order.pop(0)
            del self.cache[lru_key]

        self.cache[key] = value
        self.access_order.append(key)

class DiskCache:
    """Persistent disk-based cache"""
    def __init__(self, cache_dir: str = '.isaac_cache/disk'):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

    def get(self, key: str) -> Optional[Any]:
        cache_file = os.path.join(self.cache_dir, f"{key}.pkl")
        if os.path.exists(cache_file):
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        return None

    def set(self, key: str, value: Any):
        cache_file = os.path.join(self.cache_dir, f"{key}.pkl")
        with open(cache_file, 'wb') as f:
            pickle.dump(value, f)

class MultiLevelCache:
    """Multi-level cache: L1 (hot) â†’ L2 (warm) â†’ L3 (cold/disk)"""
    def __init__(self):
        self.l1 = LRUCache(max_size=100)      # Hot data (memory)
        self.l2 = LRUCache(max_size=1000)     # Warm data (memory)
        self.l3 = DiskCache()                  # Cold data (disk)

    def get(self, key: str) -> Optional[Any]:
        # Check L1 (fastest)
        value = self.l1.get(key)
        if value is not None:
            return value

        # Check L2
        value = self.l2.get(key)
        if value is not None:
            # Promote to L1
            self.l1.set(key, value)
            return value

        # Check L3 (disk)
        value = self.l3.get(key)
        if value is not None:
            # Promote to L2 (not L1, it's cold data)
            self.l2.set(key, value)
            return value

        return None

    def set(self, key: str, value: Any):
        # Store in L1 (hot)
        self.l1.set(key, value)
        # Also store in L3 for persistence
        self.l3.set(key, value)

    def clear(self):
        self.l1.cache.clear()
        self.l2.cache.clear()
        # Clear disk cache
        for file in os.listdir(self.l3.cache_dir):
            os.remove(os.path.join(self.l3.cache_dir, file))
```

**3.3.2: Intelligent Cache Warming (3 hours)**

**Create:** `isaac/cache/cache_warmer.py`

```python
import json
import os
from typing import List

class CacheWarmer:
    """Pre-populate cache with common queries on startup"""

    def __init__(self, cache: MultiLevelCache):
        self.cache = cache
        self.usage_log_file = '.isaac_cache/usage_log.json'

    def learn_from_usage(self, command: str):
        """Track command usage for learning"""
        if not os.path.exists(self.usage_log_file):
            usage = {}
        else:
            with open(self.usage_log_file) as f:
                usage = json.load(f)

        usage[command] = usage.get(command, 0) + 1

        with open(self.usage_log_file, 'w') as f:
            json.dump(usage, f)

    def get_common_queries(self, top_n: int = 20) -> List[str]:
        """Get most common queries from usage log"""
        if not os.path.exists(self.usage_log_file):
            return []

        with open(self.usage_log_file) as f:
            usage = json.load(f)

        # Sort by frequency
        sorted_commands = sorted(usage.items(), key=lambda x: x[1], reverse=True)
        return [cmd for cmd, count in sorted_commands[:top_n]]

    def warmup_cache(self):
        """Pre-populate cache with common queries"""
        common_queries = self.get_common_queries()

        for query in common_queries:
            # Pre-compute and cache result
            # (actual implementation depends on query type)
            pass
```

**3.3.3: Cache Invalidation Strategy (3 hours)**

**Update:** `isaac/cache/multilevel_cache.py`

```python
import time

class MultiLevelCache:
    def __init__(self, ttl: int = 3600):  # 1 hour TTL
        self.l1 = LRUCache(max_size=100)
        self.l2 = LRUCache(max_size=1000)
        self.l3 = DiskCache()
        self.ttl = ttl
        self.timestamps = {}  # Track when items were cached

    def set(self, key: str, value: Any, ttl: Optional[int] = None):
        # Store value
        self.l1.set(key, value)
        self.l3.set(key, value)

        # Track timestamp for TTL
        self.timestamps[key] = {
            'created': time.time(),
            'ttl': ttl or self.ttl
        }

    def get(self, key: str) -> Optional[Any]:
        # Check if expired
        if key in self.timestamps:
            created = self.timestamps[key]['created']
            ttl = self.timestamps[key]['ttl']
            if time.time() - created > ttl:
                # Expired - invalidate
                self.invalidate(key)
                return None

        # Normal cache lookup
        return self._get_from_levels(key)

    def invalidate(self, key: str):
        """Manually invalidate a cache entry"""
        if key in self.l1.cache:
            del self.l1.cache[key]
        if key in self.l2.cache:
            del self.l2.cache[key]
        # Remove from disk
        cache_file = os.path.join(self.l3.cache_dir, f"{key}.pkl")
        if os.path.exists(cache_file):
            os.remove(cache_file)
        if key in self.timestamps:
            del self.timestamps[key]

    def invalidate_pattern(self, pattern: str):
        """Invalidate all keys matching pattern"""
        import fnmatch
        keys_to_invalidate = [
            key for key in self.l1.cache.keys()
            if fnmatch.fnmatch(key, pattern)
        ]
        for key in keys_to_invalidate:
            self.invalidate(key)
```

**3.3.4: Cache Performance Testing (2 hours)**

**Create:** `tests/test_multilevel_cache.py`

```python
import pytest
import time
from isaac.cache.multilevel_cache import MultiLevelCache

def test_cache_hit_performance():
    cache = MultiLevelCache()

    # Warm up
    cache.set("test_key", "test_value")

    # Measure L1 hit (should be <1ms)
    start = time.time()
    for _ in range(1000):
        value = cache.get("test_key")
    elapsed = time.time() - start

    avg_time = elapsed / 1000 * 1000  # ms
    assert avg_time < 0.1, f"L1 cache too slow: {avg_time}ms per access"

def test_cache_promotion():
    cache = MultiLevelCache()

    # Set value
    cache.set("test", "value")

    # Clear L1 to simulate eviction
    cache.l1.cache.clear()

    # Get should promote from L2 to L1
    value = cache.get("test")
    assert value == "value"
    assert "test" in cache.l1.cache

def test_cache_ttl():
    cache = MultiLevelCache(ttl=1)  # 1 second TTL

    cache.set("test", "value")
    assert cache.get("test") == "value"

    time.sleep(1.5)
    assert cache.get("test") is None  # Expired
```

**Commit:**
```bash
git add isaac/cache/ tests/test_multilevel_cache.py
git commit -m "feat: Implement multi-level caching (L1/L2/L3 + TTL)"
```

**Success:** Cache hit rate >60%, sub-millisecond L1 access

---

### Task 3.4: Optimize Data Structures (8 hours)

**Goal:** 5-10x faster for specific operations

**3.4.1: Profile Hot Paths (2 hours)**

```bash
# Install profiling tools
pip install py-spy memory_profiler line_profiler

# Profile command routing
python -m cProfile -o profile.stats -m isaac "/help"

# Analyze
python -c "import pstats; p = pstats.Stats('profile.stats'); p.sort_stats('cumtime'); p.print_stats(20)"

# Flamegraph
py-spy record -o flamegraph.svg -- python -m isaac "/help"
```

**Identify hot paths** (functions consuming most time)

**3.4.2: Optimize Identified Bottlenecks (4 hours)**

**Common optimizations:**

**Example 1: Lists â†’ Sets for lookups**

**Before:**
```python
# isaac/core/tier_validator.py
TIER_4_COMMANDS = ['sudo', 'rm -rf', 'format', ...]  # List - O(n)

def validate(self, cmd: str) -> int:
    if cmd in TIER_4_COMMANDS:  # O(n) lookup!
        return 4
```

**After:**
```python
TIER_4_COMMANDS = {'sudo', 'rm -rf', 'format', ...}  # Set - O(1)

def validate(self, cmd: str) -> int:
    if cmd in TIER_4_COMMANDS:  # O(1) lookup!
        return 4
```

**Example 2: Use defaultdict for counting**

**Before:**
```python
counts = {}
for item in items:
    if item in counts:
        counts[item] += 1
    else:
        counts[item] = 1
```

**After:**
```python
from collections import defaultdict
counts = defaultdict(int)
for item in items:
    counts[item] += 1
```

**Example 3: Reduce object allocation**

**Before:**
```python
def process_items(items):
    results = []
    for item in items:
        result = expensive_operation(item)
        results.append(result)
    return results
```

**After:**
```python
def process_items(items):
    # Pre-allocate if size known
    results = [None] * len(items)
    for i, item in enumerate(items):
        results[i] = expensive_operation(item)
    return results
```

**3.4.3: Verify Optimizations (2 hours)**

Re-run profiling after optimizations:

```bash
python -m cProfile -o profile_after.stats -m isaac "/help"

# Compare before/after
python compare_profiles.py profile.stats profile_after.stats
```

**Benchmark specific operations:**

```python
import timeit

# Before optimization
before_time = timeit.timeit(
    'cmd in TIER_4_COMMANDS_LIST',
    setup='TIER_4_COMMANDS_LIST = ["sudo", "rm", ...]',
    number=100000
)

# After optimization
after_time = timeit.timeit(
    'cmd in TIER_4_COMMANDS_SET',
    setup='TIER_4_COMMANDS_SET = {"sudo", "rm", ...}',
    number=100000
)

print(f"Speedup: {before_time/after_time:.1f}x")
```

**Commit:**
```bash
git add isaac/
git commit -m "perf: Optimize hot paths (sets, defaultdict, object pooling) - 5-10x faster"
```

**Success:** Hot paths 5-10x faster

---

### Task 3.5: Memory Optimization (8 hours)

**Goal:** Reduce memory footprint 20-30%

**3.5.1: Memory Profiling (2 hours)**

```bash
pip install memory_profiler

# Profile memory usage
python -m memory_profiler isaac/__main__.py

# Or use memray for detailed analysis
pip install memray
memray run isaac/__main__.py
memray flamegraph memray-output.bin
```

**Identify memory hotspots**

**3.5.2: Reduce Memory Footprint (4 hours)**

**Optimization 1: Use __slots__ for frequent objects**

**Before:**
```python
class CommandResult:
    def __init__(self, success, data, error):
        self.success = success
        self.data = data
        self.error = error
```

**After:**
```python
class CommandResult:
    __slots__ = ['success', 'data', 'error']  # Reduces memory ~40%

    def __init__(self, success, data, error):
        self.success = success
        self.data = data
        self.error = error
```

**Optimization 2: Clear large data structures promptly**

```python
def process_large_dataset(data):
    results = expensive_computation(data)

    # Clear input data immediately if no longer needed
    del data

    return results
```

**Optimization 3: Use generators for large sequences**

**Before:**
```python
def get_all_files(directory):
    files = []  # Loads all into memory
    for root, dirs, filenames in os.walk(directory):
        for filename in filenames:
            files.append(os.path.join(root, filename))
    return files
```

**After:**
```python
def get_all_files(directory):
    # Generator - yields one at a time
    for root, dirs, filenames in os.walk(directory):
        for filename in filenames:
            yield os.path.join(root, filename)
```

**Optimization 4: Optimize cache memory usage**

```python
class MultiLevelCache:
    def __init__(self, max_memory_mb: int = 100):
        self.max_memory = max_memory_mb * 1024 * 1024
        self.current_memory = 0

    def set(self, key: str, value: Any):
        import sys
        value_size = sys.getsizeof(value)

        # Check if adding would exceed limit
        if self.current_memory + value_size > self.max_memory:
            self._evict_until_fits(value_size)

        # Store value
        self.l1.set(key, value)
        self.current_memory += value_size
```

**3.5.3: Verify Memory Improvements (2 hours)**

```bash
# Re-profile memory
python -m memory_profiler isaac/__main__.py

# Compare before/after
# Should see 20-30% reduction
```

**Commit:**
```bash
git add isaac/
git commit -m "perf: Optimize memory usage (__slots__, generators, cache limits) - 20-30% reduction"
```

**Success:** Memory usage reduced 20-30%

---

### Task 3.6: Comprehensive Performance Testing (12 hours)

**Goal:** Verify all optimizations, create benchmark suite

**3.6.1: Create Benchmark Suite (4 hours)**

**Create:** `tests/benchmarks/benchmark_suite.py`

```python
import time
import pytest
from isaac.core.command_router import CommandRouter
from isaac.ai.router import AIRouter

class BenchmarkSuite:
    def benchmark_command_routing(self):
        """Measure command routing speed"""
        router = CommandRouter()

        start = time.time()
        for _ in range(1000):
            router.route_command('/help', {})
        elapsed = time.time() - start

        avg_time = elapsed / 1000 * 1000  # ms
        print(f"Command routing: {avg_time:.2f}ms per command")
        assert avg_time < 3, f"Too slow: {avg_time}ms"

    def benchmark_alias_translation(self):
        """Measure alias lookup speed"""
        from isaac.crossplatform.unix_alias_translator import UnixAliasTranslator
        translator = UnixAliasTranslator()

        start = time.time()
        for _ in range(1000):
            translator.translate('ls -la', 'powershell')
        elapsed = time.time() - start

        avg_time = elapsed / 1000 * 1000  # ms
        print(f"Alias translation: {avg_time:.2f}ms per lookup")
        assert avg_time < 1, f"Too slow: {avg_time}ms"

    def benchmark_ai_queries(self):
        """Measure AI query latency"""
        router = AIRouter()

        # Single query
        start = time.time()
        router.query("test prompt")
        single_time = time.time() - start

        # Batch query
        prompts = ["prompt " + str(i) for i in range(5)]
        start = time.time()
        import asyncio
        asyncio.run(router.batch_query(prompts))
        batch_time = time.time() - start

        print(f"Single query: {single_time:.2f}s")
        print(f"Batch query (5): {batch_time:.2f}s")
        print(f"Speedup: {(single_time * 5) / batch_time:.1f}x")

    def benchmark_startup(self):
        """Measure startup time"""
        import subprocess

        start = time.time()
        subprocess.run(['python', '-m', 'isaac', '--version'], capture_output=True)
        elapsed = time.time() - start

        print(f"Startup time: {elapsed:.2f}s")
        assert elapsed < 1, f"Startup too slow: {elapsed}s"

if __name__ == '__main__':
    suite = BenchmarkSuite()
    suite.benchmark_command_routing()
    suite.benchmark_alias_translation()
    suite.benchmark_ai_queries()
    suite.benchmark_startup()
```

**3.6.2: Run Comprehensive Benchmarks (4 hours)**

```bash
# Run benchmark suite
python tests/benchmarks/benchmark_suite.py > benchmark_results.txt

# Compare with Phase 2 baseline
python compare_benchmarks.py baseline_phase2.txt benchmark_results.txt
```

**3.6.3: Profile End-to-End Workflows (4 hours)**

Test real-world scenarios:

```python
# tests/benchmarks/workflow_benchmarks.py

def benchmark_workflow_read_analyze():
    """Read file + AI analysis workflow"""
    # Simulates: /read file.py | /analyze
    pass

def benchmark_workflow_search_edit():
    """Search + Edit workflow"""
    # Simulates: /grep pattern *.py | /edit
    pass

def benchmark_workflow_batch_ai():
    """Batch AI queries workflow"""
    # Simulates: multiple AI queries in sequence
    pass
```

**Commit:**
```bash
git add tests/benchmarks/
git commit -m "test: Add comprehensive performance benchmark suite"
```

**Success:** All benchmarks pass, performance targets met

---

### Task 3.7: Phase 3 Documentation (12 hours)

**Create these documents:**

1. **PERFORMANCE_OPTIMIZATION_GUIDE.md** (4 hours)
   - Document optimization techniques
   - Before/after benchmarks
   - Best practices

2. **CACHING_STRATEGY_GUIDE.md** (4 hours)
   - Cache architecture
   - Configuration options
   - Cache management

3. **PERFORMANCE_MONITORING_GUIDE.md** (2 hours)
   - How to measure performance
   - Profiling tools
   - Troubleshooting

4. **PHASE_3_COMPLETION_REPORT.md** (2 hours)
   - Performance gains summary
   - Benchmark comparisons
   - Resource utilization analysis

**Commit:**
```bash
git add docs/
git commit -m "docs: Add Phase 3 performance optimization documentation"
```

---

## âœ… SUCCESS CRITERIA

Phase 3 complete when:

- [x] Async AI operations implemented (10-20x batch)
- [x] Parallel plugin loading (60-80% faster startup)
- [x] Multi-level caching (50-70% cost reduction)
- [x] Hot paths optimized (5-10x faster)
- [x] Memory usage reduced 20-30%
- [x] All benchmarks pass
- [x] Startup time <1s
- [x] Command resolution <3ms
- [x] Alias lookup <1ms
- [x] Documentation complete

---

## ðŸ“Š EXPECTED IMPROVEMENTS

| Metric | Before (Phase 2) | After (Phase 3) | Improvement |
|--------|------------------|-----------------|-------------|
| **Overall Health** | 8.0/10 | 8.5/10 | +6% |
| Command Resolution | 3-10ms | 1-3ms | **3-10x faster** |
| Alias Lookup | 50-200ms | <1ms | **50-200x faster** |
| Plugin Load | 1-5s | <1s | **60-80% faster** |
| AI Query (batch 5) | 10-25s | 2-5s | **5-10x faster** |
| Startup Time | 2-5s | <1s | **60-80% faster** |
| Memory Usage | Baseline | -20-30% | **Lower footprint** |
| **Overall Performance** | Baseline | **5-10x faster** | âœ… |

---

## ðŸ“š REFERENCE DOCUMENTS

- **IMPLEMENTATION_ROADMAP.md** - Phase 3 full details
- **QUICK_WINS.md** - Performance quick wins
- **EXECUTIVE_SUMMARY.md** - Strategic context

---

## â­ï¸ NEXT PHASE (Optional)

After Phase 3 completion:
- **Phase 4 (Enhancement)** - See PHASE_4_KICKOFF.md
- Focus: Cloud integration, plugin marketplace, web interface
- Timeline: 7-8 weeks (optional - can defer to v2.1)

---

**READY TO OPTIMIZE!** âš¡

Start with Task 3.1 (Implement Async AI Calls).
